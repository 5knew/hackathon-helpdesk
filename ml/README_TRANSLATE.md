# Перевод и преобразование датасета

## Описание

Скрипт `translate_and_transform_dataset.py` переводит датасет на русский и казахский языки, создавая дубликаты каждой строки для обоих языков.

## Установка зависимостей

```bash
pip install deep-translator tqdm
```

Или обновите requirements.txt:
```bash
pip install -r requirements.txt
```

## Использование

### Базовое использование (перевод всего датасета)

```bash
python translate_and_transform_dataset.py
```

Это:
- Загрузит `datasets/aa_dataset-tickets-multi-lang-5-2-50-version.csv`
- Переведет все тексты на русский и казахский
- Создаст дубликаты для каждого языка
- Сохранит результат в `datasets/dataset_translated_ru_kz.csv`

### Тестирование на выборке

Для быстрого тестирования используйте выборку:

```bash
python translate_and_transform_dataset.py --sample 100
```

Это обработает только 100 строк.

### Указание входного/выходного файла

```bash
python translate_and_transform_dataset.py \
  --input datasets/dataset_preprocessed.csv \
  --output datasets/dataset_ru_kz.csv
```

### Без перевода (только дублирование)

Если тексты уже переведены, можно просто создать дубликаты:

```bash
python translate_and_transform_dataset.py --no-translate
```

## Особенности

1. **Кэширование переводов** - переводы сохраняются в `translation_cache.json` для ускорения повторных запусков
2. **Обработка длинных текстов** - длинные тексты разбиваются на части
3. **Определение языка** - автоматически определяет, нужно ли переводить
4. **Прогресс-бар** - показывает прогресс обработки

## Результат

После обработки вы получите датасет, где:
- Каждая исходная строка дублируется 2 раза (для RU и KZ)
- Все тексты переведены на соответствующий язык
- Колонка `language` содержит 'ru' или 'kz'

## Пример использования после перевода

После перевода можно использовать стандартный pipeline:

```bash
# 1. Подготовка датасета (если нужно)
python prepare_dataset.py

# 2. Обучение моделей
python train_classifiers.py
```

## Примечания

- Перевод может занять много времени для больших датасетов
- Рекомендуется сначала протестировать на выборке (`--sample 100`)
- Кэш переводов ускорит повторные запуски
- Для очень больших датасетов можно обрабатывать частями

