"""
–ú–æ–¥—É–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ç–∏–ø–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç FAISS –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ —à–∞–±–ª–æ–Ω–∞–º –æ—Ç–≤–µ—Ç–æ–≤
"""

import json
import os
import hashlib
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import Optional, Dict, Tuple
import joblib


class AutoReplyService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ç–∏–ø–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã"""
    
    def __init__(self, responses_path: str = "responses.json",
                 model_path: str = None,
                 similarity_threshold: float = 0.65,
                 index_path: str = "models/faiss_index.bin",
                 metadata_path: str = "models/faiss_index_meta.json"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –∞–≤—Ç–æ–æ—Ç–≤–µ—Ç–∞
        
        Args:
            responses_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —à–∞–±–ª–æ–Ω–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤
            model_path: –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ sentence-transformers (–µ—Å–ª–∏ None, –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ models/)
            similarity_threshold: –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –∞–≤—Ç–æ–æ—Ç–≤–µ—Ç–∞ (0-1)
            index_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ FAISS –∏–Ω–¥–µ–∫—Å–∞
            metadata_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–∞
        """
        self.similarity_threshold = similarity_threshold
        self.responses_path = responses_path
        self.index_path = index_path
        self.metadata_path = metadata_path
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        if model_path is None:
            model_path = "models/sentence_transformer_model"
        
        if os.path.exists(model_path):
            self.model = SentenceTransformer(model_path)
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            print(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {model_path}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é...")
            self.model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —à–∞–±–ª–æ–Ω–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤
        self.responses = self._load_responses()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞
        self.index = None
        self.response_texts = []
        self.response_metadata = []
        if not self._load_cached_index():
            self._build_index()
    
    def _load_responses(self) -> list:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        if not os.path.exists(self.responses_path):
            raise FileNotFoundError(f"–§–∞–π–ª {self.responses_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        
        with open(self.responses_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return data.get('responses', [])
    
    def _build_index(self):
        """–°–æ–∑–¥–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å –¥–ª—è –≤—Å–µ—Ö —à–∞–±–ª–æ–Ω–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤"""
        print("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –∞–≤—Ç–æ–æ—Ç–≤–µ—Ç–æ–≤...")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏ –∫–∞–∑–∞—Ö—Å–∫–æ–º
        texts = []
        metadata = []
        
        for resp in self.responses:
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä—É—Å—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
            if 'ru' in resp:
                texts.append(resp['ru'])
                metadata.append({
                    'id': resp['id'],
                    'category': resp.get('category', ''),
                    'language': 'ru',
                    'keywords': resp.get('keywords', [])
                })
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∑–∞—Ö—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
            if 'kz' in resp:
                texts.append(resp['kz'])
                metadata.append({
                    'id': resp['id'],
                    'category': resp.get('category', ''),
                    'language': 'kz',
                    'keywords': resp.get('keywords', [])
                })
        
        if not texts:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —à–∞–±–ª–æ–Ω–∞ –æ—Ç–≤–µ—Ç–∞!")
        
        self.response_texts = texts

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö —à–∞–±–ª–æ–Ω–æ–≤
        print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts)} —à–∞–±–ª–æ–Ω–æ–≤...")
        embeddings = self.model.encode(texts, show_progress_bar=True, batch_size=32)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        faiss.normalize_L2(embeddings)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å
        self.index.add(embeddings.astype('float32'))
        self.response_metadata = metadata

        print(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {dimension}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
        faiss.write_index(self.index, self.index_path)
        with open(self.metadata_path, "w", encoding="utf-8") as f:
            json.dump({
                "responses_hash": self._responses_hash(),
                "response_texts": self.response_texts,
                "response_metadata": self.response_metadata
            }, f, ensure_ascii=False, indent=2)
        print(f"üíæ –ò–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.index_path} –∏ {self.metadata_path}")
    
    def _detect_language(self, text: str) -> str:
        """
        –ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞
        (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è –±–∏–±–ª–∏–æ—Ç–µ–∫—É langdetect)
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∫–∞–∑–∞—Ö—Å–∫–∏–º/—Ä—É—Å—Å–∫–∏–º
        kz_chars = ['”ô', '“ì', '“õ', '“£', '”©', '“±', '“Ø', '“ª', '—ñ']
        has_kz_chars = any(char in text.lower() for char in kz_chars)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–∞–∑–∞—Ö—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã, –≤–µ—Ä–æ—è—Ç–Ω–æ –∫–∞–∑–∞—Ö—Å–∫–∏–π
        if has_kz_chars:
            return 'kz'
        
        # –ò–Ω–∞—á–µ —Å—á–∏—Ç–∞–µ–º —Ä—É—Å—Å–∫–∏–º (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        return 'ru'
    
    def find_best_response(self, query: str, category: str = None, 
                          language: str = None, top_k: int = 3) -> Optional[Dict]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        
        Args:
            query: —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            category: –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–∏–∫–µ—Ç–∞ (–¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
            language: —è–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞ ('ru' –∏–ª–∏ 'kz'), –µ—Å–ª–∏ None - –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ–¥—Ö–æ–¥—è—â–∏–π
        """
        if self.index is None or len(self.response_texts) == 0:
            return None
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        if language is None:
            language = self._detect_language(query)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query])[0]
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        faiss.normalize_L2(query_embedding)
        
        # –ü–æ–∏—Å–∫ –≤ FAISS –∏–Ω–¥–µ–∫—Å–µ
        k = min(top_k * 2, self.index.ntotal)  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        similarities, indices = self.index.search(query_embedding, k)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —è–∑—ã–∫—É –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        best_match = None
        best_similarity = 0.0
        
        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
            metadata = self.response_metadata[idx]
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —è–∑—ã–∫–∞
            if metadata['language'] != language:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)
            if category and metadata['category'] != category:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
            if similarity >= self.similarity_threshold and similarity > best_similarity:
                best_similarity = similarity
                best_match = {
                    'response_id': metadata['id'],
                    'text': self.response_texts[idx],
                    'similarity': float(similarity),
                    'category': metadata['category'],
                    'language': metadata['language'],
                    'keywords': metadata.get('keywords', [])
                }
        
        return best_match
    
    def can_auto_reply(self, query: str, problem_type: str, 
                      category: str = None) -> Tuple[bool, Optional[Dict]]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –º–æ–∂–Ω–æ –ª–∏ –¥–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç
        
        Args:
            query: —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            problem_type: —Ç–∏–ø –ø—Ä–æ–±–ª–µ–º—ã ('–¢–∏–ø–æ–≤–æ–π' –∏–ª–∏ '–°–ª–æ–∂–Ω—ã–π')
            category: –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–∏–∫–µ—Ç–∞
        
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (–º–æ–∂–Ω–æ_–ª–∏_–æ—Ç–≤–µ—Ç–∏—Ç—å, –æ—Ç–≤–µ—Ç_–∏–ª–∏_None)
        """
        # –°–ª–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        if problem_type == '–°–ª–æ–∂–Ω—ã–π':
            return False, None
        
        # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç
        best_response = self.find_best_response(query, category=category)
        
        if best_response and best_response['similarity'] >= self.similarity_threshold:
            return True, best_response
        
        return False, None
    
    def get_auto_reply(self, query: str, problem_type: str, 
                      category: str = None, language: str = None) -> Dict:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        
        Args:
            query: —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            problem_type: —Ç–∏–ø –ø—Ä–æ–±–ª–µ–º—ã
            category: –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–∏–∫–µ—Ç–∞
            language: —è–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –∞–≤—Ç–æ–æ—Ç–≤–µ—Ç–∞
        """
        can_reply, response = self.can_auto_reply(query, problem_type, category)
        
        if can_reply and response:
            return {
                'can_auto_reply': True,
                'response_text': response['text'],
                'response_id': response['response_id'],
                'similarity': response['similarity'],
                'category': response['category'],
                'language': response['language']
            }
        else:
            # –ï—Å–ª–∏ response –µ—Å—Ç—å, –Ω–æ —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∏–∑–∫–∞—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ—ë
            similarity = response['similarity'] if response and 'similarity' in response else 0.0
            return {
                'can_auto_reply': False,
                'reason': '–°–ª–æ–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å' if problem_type == '–°–ª–æ–∂–Ω—ã–π' else '–ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ–¥—Ö–æ–¥—è—â–∏–π —à–∞–±–ª–æ–Ω',
                'similarity': similarity
            }


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
def save_index(service: AutoReplyService, index_path: str = "models/faiss_index.bin"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫"""
    os.makedirs(os.path.dirname(index_path), exist_ok=True)
    faiss.write_index(service.index, index_path)
    print(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {index_path}")


def load_index(index_path: str = "models/faiss_index.bin") -> Optional[faiss.Index]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å —Å –¥–∏—Å–∫–∞"""
    if os.path.exists(index_path):
        return faiss.read_index(index_path)
    return None


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è
    print("=" * 60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–£–õ–Ø –ê–í–¢–û–û–¢–í–ï–¢–ê")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞
    service = AutoReplyService()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        ("–ö–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å?", "–¢–∏–ø–æ–≤–æ–π", "–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã"),
        ("–ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –æ–ø–ª–∞—á–∏–≤–∞—Ç—å —Å—á–µ—Ç?", "–¢–∏–ø–æ–≤–æ–π", "–ë–∏–ª–ª–∏–Ω–≥ –∏ –ø–ª–∞—Ç–µ–∂–∏"),
        ("–ö–∞–∫–∏–µ —Å–ø–æ—Å–æ–±—ã –æ–ø–ª–∞—Ç—ã –≤—ã –ø—Ä–∏–Ω–∏–º–∞–µ—Ç–µ?", "–¢–∏–ø–æ–≤–æ–π", "–ë–∏–ª–ª–∏–Ω–≥ –∏ –ø–ª–∞—Ç–µ–∂–∏"),
        ("–ù–µ –º–æ–≥—É –≤–æ–π—Ç–∏ –≤ —Å–∏—Å—Ç–µ–º—É", "–¢–∏–ø–æ–≤–æ–π", "–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã"),
        ("–°–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞", "–°–ª–æ–∂–Ω—ã–π", "IT –ø–æ–¥–¥–µ—Ä–∂–∫–∞"),
    ]
    
    print("\n" + "=" * 60)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("=" * 60)
    
    for query, problem_type, category in test_queries:
        print(f"\nüìù –ó–∞–ø—Ä–æ—Å: {query}")
        print(f"   –¢–∏–ø: {problem_type}, –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}")
        
        result = service.get_auto_reply(query, problem_type, category)
        
        if result['can_auto_reply']:
            print(f"   ‚úÖ –ê–≤—Ç–æ–æ—Ç–≤–µ—Ç –≤–æ–∑–º–æ–∂–µ–Ω (similarity: {result['similarity']:.3f})")
            print(f"   üìÑ –û—Ç–≤–µ—Ç: {result['response_text'][:100]}...")
        else:
            print(f"   ‚ùå –ê–≤—Ç–æ–æ—Ç–≤–µ—Ç –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω: {result['reason']}")
            if result.get('similarity', 0) > 0:
                print(f"   (–õ—É—á—à–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {result['similarity']:.3f})")

    def _responses_hash(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Å—É–º–º—É —Ñ–∞–π–ª–∞ —Å —à–∞–±–ª–æ–Ω–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤"""
        hasher = hashlib.md5()
        with open(self.responses_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()

    def _load_cached_index(self) -> bool:
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π FAISS –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –æ–Ω–∏
        —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç–µ–∫—É—â–µ–º—É —Ñ–∞–π–ª—É –æ—Ç–≤–µ—Ç–æ–≤.
        """
        if not (os.path.exists(self.index_path) and os.path.exists(self.metadata_path)):
            return False

        try:
            with open(self.metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)

            if metadata.get("responses_hash") != self._responses_hash():
                return False

            self.response_texts = metadata.get("response_texts", [])
            self.response_metadata = metadata.get("response_metadata", [])
            self.index = faiss.read_index(self.index_path)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π FAISS –∏–Ω–¥–µ–∫—Å –∏–∑ {self.index_path}")
            return True
        except Exception as exc:  # pragma: no cover - –∑–∞—â–∏—Ç–Ω—ã–π –±–ª–æ–∫
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {exc}")
            return False
