"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
–°–æ–≥–ª–∞—Å–Ω–æ –¢–ó:
- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
- –ß–µ—Ç–∫–∏–µ —Å—Ö–µ–º—ã –º–µ—Ç–æ–∫ (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, —Ç–∏–ø –ø—Ä–æ–±–ª–µ–º—ã)
- –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–º–µ—Ç—á–∏–∫–æ–≤
- –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ RU/KZ
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Set
import os
from tqdm import tqdm
import json
from collections import Counter
import hashlib

try:
    from deep_translator import GoogleTranslator
    TRANSLATOR_AVAILABLE = True
except ImportError:
    TRANSLATOR_AVAILABLE = False
    print("‚ö†Ô∏è  deep-translator –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install deep-translator")


class DatasetNormalizer:
    """–ö–ª–∞—Å—Å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    # –ß–µ—Ç–∫–∏–µ —Å—Ö–µ–º—ã –º–µ—Ç–æ–∫ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
    CATEGORY_SCHEMA = {
        'IT –ø–æ–¥–¥–µ—Ä–∂–∫–∞': [
            'IT', 'IT Support', 'IT –ø–æ–¥–¥–µ—Ä–∂–∫–∞', 
            'Technical Support', '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞',
            '–°–±–æ–∏ –∏ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'
        ],
        '–ë–∏–ª–ª–∏–Ω–≥ –∏ –ø–ª–∞—Ç–µ–∂–∏': [
            'Billing', 'Payment', 'Invoice', '–ë–∏–ª–ª–∏–Ω–≥', '–ü–ª–∞—Ç–µ–∂–∏',
            'Billing and Payments', '–ë–∏–ª–ª–∏–Ω–≥ –∏ –ø–ª–∞—Ç–µ–∂–∏'
        ],
        '–ö–ª–∏–µ–Ω—Ç—Å–∫–∏–π —Å–µ—Ä–≤–∏—Å': [
            'Customer Service', 'Support', '–ö–ª–∏–µ–Ω—Ç—Å–∫–∏–π —Å–µ—Ä–≤–∏—Å',
            '–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–∞', 'Product Support'
        ],
        'HR': [
            'HR', 'Human Resources', '–ö–∞–¥—Ä—ã', 'Human Resources'
        ],
        '–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã': [
            'General', 'FAQ', '–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã', '–í–æ–ø—Ä–æ—Å—ã',
            'General Inquiry', '–ü—Ä–æ–¥–∞–∂–∏', 'Sales',
            '–í–æ–∑–≤—Ä–∞—Ç—ã –∏ –æ–±–º–µ–Ω—ã', 'Returns and Exchanges'
        ]
    }
    
    PRIORITY_SCHEMA = {
        '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π': ['Critical', 'P1', '–í—ã—Å–æ–∫–∏–π', 'High', '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π'],
        '–í—ã—Å–æ–∫–∏–π': ['High', 'P2', '–°—Ä–µ–¥–Ω–∏–π', 'Medium', '–í—ã—Å–æ–∫–∏–π'],
        '–°—Ä–µ–¥–Ω–∏–π': ['Medium', 'P3', '–ù–∏–∑–∫–∏–π', 'Low', '–°—Ä–µ–¥–Ω–∏–π'],
        '–ù–∏–∑–∫–∏–π': ['Low', 'P4', '–ù–∏–∑–∫–∏–π']
    }
    
    PROBLEM_TYPE_SCHEMA = {
        '–¢–∏–ø–æ–≤–æ–π': ['Typical', 'Standard', 'FAQ', 'Common', '–¢–∏–ø–æ–≤–æ–π', '–û–±—ã—á–Ω—ã–π'],
        '–°–ª–æ–∂–Ω—ã–π': ['Complex', 'Critical', 'Urgent', '–°–ª–æ–∂–Ω—ã–π', '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π']
    }
    
    def __init__(self):
        self.duplicate_cache = {}
        self.normalization_stats = {
            'duplicates_removed': 0,
            'categories_normalized': 0,
            'priorities_normalized': 0,
            'problem_types_normalized': 0
        }
    
    def normalize_category(self, category: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ"""
        if pd.isna(category) or not category:
            return '–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã'
        
        category_str = str(category).strip()
        original = category_str
        
        # –ü–æ–∏—Å–∫ –≤ —Å—Ö–µ–º–µ (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
        for normalized, variants in self.CATEGORY_SCHEMA.items():
            if category_str == normalized:
                return normalized
            if category_str in variants:
                self.normalization_stats['categories_normalized'] += 1
                return normalized
        
        # –ü–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é (–±–æ–ª–µ–µ –≥–∏–±–∫–∏–π)
        category_lower = category_str.lower()
        for normalized, variants in self.CATEGORY_SCHEMA.items():
            for variant in variants:
                if variant.lower() in category_lower or category_lower in variant.lower():
                    if category_str != normalized:
                        self.normalization_stats['categories_normalized'] += 1
                    return normalized
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        category_mapping = {
            '–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–∞': '–ö–ª–∏–µ–Ω—Ç—Å–∫–∏–π —Å–µ—Ä–≤–∏—Å',
            'Product Support': '–ö–ª–∏–µ–Ω—Ç—Å–∫–∏–π —Å–µ—Ä–≤–∏—Å',
            '–í–æ–∑–≤—Ä–∞—Ç—ã –∏ –æ–±–º–µ–Ω—ã': '–ö–ª–∏–µ–Ω—Ç—Å–∫–∏–π —Å–µ—Ä–≤–∏—Å',
            'Returns and Exchanges': '–ö–ª–∏–µ–Ω—Ç—Å–∫–∏–π —Å–µ—Ä–≤–∏—Å',
            '–ü—Ä–æ–¥–∞–∂–∏': '–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã',
            'Sales': '–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã',
            'Sales and Pre-Sales': '–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã',
        }
        
        if category_str in category_mapping:
            self.normalization_stats['categories_normalized'] += 1
            return category_mapping[category_str]
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        return category_str
    
    def normalize_priority(self, priority: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ"""
        if pd.isna(priority) or not priority:
            return '–°—Ä–µ–¥–Ω–∏–π'
        
        priority_str = str(priority).strip()
        
        # –ü–æ–∏—Å–∫ –≤ —Å—Ö–µ–º–µ
        for normalized, variants in self.PRIORITY_SCHEMA.items():
            if priority_str in variants or any(v.lower() in priority_str.lower() for v in variants):
                if priority_str != normalized:
                    self.normalization_stats['priorities_normalized'] += 1
                return normalized
        
        return priority_str
    
    def normalize_problem_type(self, problem_type: str, priority: str = None) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–∏–ø –ø—Ä–æ–±–ª–µ–º—ã —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ"""
        if pd.isna(problem_type) or not problem_type:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
            if priority and '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π' in str(priority):
                return '–°–ª–æ–∂–Ω—ã–π'
            return '–¢–∏–ø–æ–≤–æ–π'
        
        problem_type_str = str(problem_type).strip()
        
        # –ü–æ–∏—Å–∫ –≤ —Å—Ö–µ–º–µ
        for normalized, variants in self.PROBLEM_TYPE_SCHEMA.items():
            if problem_type_str in variants or any(v.lower() in problem_type_str.lower() for v in variants):
                if problem_type_str != normalized:
                    self.normalization_stats['problem_types_normalized'] += 1
                return normalized
        
        return problem_type_str
    
    def create_text_hash(self, text: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ö—ç—à —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        if pd.isna(text) or not text:
            return ''
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç (—É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
        normalized = str(text).lower().strip().replace('\n', ' ').replace('\r', ' ')
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        normalized = ' '.join(normalized.split())
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def find_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ù–∞—Ö–æ–¥–∏—Ç –∏ —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã"""
        print("\n   –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º —Ö—ç—à –¥–ª—è subject + body
        df['text_hash'] = (df['subject'].fillna('').astype(str) + ' ' + 
                          df['body'].fillna('').astype(str)).apply(self.create_text_hash)
        
        initial_count = len(df)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞–≤–ª—è—è –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å
        df_clean = df.drop_duplicates(subset=['text_hash'], keep='first')
        
        duplicates_removed = initial_count - len(df_clean)
        self.normalization_stats['duplicates_removed'] = duplicates_removed
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É
        if 'text_hash' in df_clean.columns:
            df_clean = df_clean.drop(columns=['text_hash'])
        
        print(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed}")
        
        return df_clean


class DatasetTranslator:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    def __init__(self, use_cache=True):
        self.use_cache = use_cache
        self.cache_file = "translation_cache.json"
        self.cache = self._load_cache() if use_cache else {}
        
        if TRANSLATOR_AVAILABLE:
            self.translator_ru = GoogleTranslator(source='auto', target='ru')
            self.translator_kz = GoogleTranslator(source='auto', target='kk')
            print("‚úÖ –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        else:
            self.translator_ru = None
            self.translator_kz = None
            print("‚ö†Ô∏è  –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    def _load_cache(self) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –ø–µ—Ä–µ–≤–æ–¥–æ–≤"""
        if self.use_cache:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
    
    def translate_text(self, text: str, target_lang: str = 'ru') -> str:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ü–µ–ª–µ–≤–æ–π —è–∑—ã–∫"""
        if not text or pd.isna(text) or str(text).strip() == '':
            return ''
        
        text = str(text).strip()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = f"{text}_{target_lang}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # –ï—Å–ª–∏ —É–∂–µ –Ω–∞ –Ω—É–∂–Ω–æ–º —è–∑—ã–∫–µ
        if target_lang == 'ru' and self._is_russian(text):
            result = text
        elif target_lang == 'kz' and self._is_kazakh(text):
            result = text
        else:
            # –ü–µ—Ä–µ–≤–æ–¥
            try:
                if TRANSLATOR_AVAILABLE and self.translator_ru and self.translator_kz:
                    if target_lang == 'ru':
                        result = self.translator_ru.translate(text[:4500])  # –õ–∏–º–∏—Ç API
                    elif target_lang == 'kz':
                        result = self.translator_kz.translate(text[:4500])
                    else:
                        result = text
                else:
                    result = text
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
                result = text
        
        self.cache[cache_key] = result
        return result
    
    def _is_russian(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Ä—É—Å—Å–∫–∏–º"""
        return any('\u0400' <= char <= '\u04FF' for char in text) and not self._is_kazakh(text)
    
    def _is_kazakh(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –∫–∞–∑–∞—Ö—Å–∫–∏–º"""
        kz_chars = ['”ô', '“ì', '“õ', '“£', '”©', '“±', '“Ø', '“ª', '—ñ']
        return any(char in text.lower() for char in kz_chars)


def create_labeling_instructions() -> str:
    """–°–æ–∑–¥–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–º–µ—Ç—á–∏–∫–æ–≤"""
    instructions = """
# –ò–ù–°–¢–†–£–ö–¶–ò–ò –î–õ–Ø –†–ê–ó–ú–ï–¢–ß–ò–ö–û–í

## –°—Ö–µ–º—ã –º–µ—Ç–æ–∫

### 1. –ö–ê–¢–ï–ì–û–†–ò–Ø (category)

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¢–û–õ–¨–ö–û —Å–ª–µ–¥—É—é—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:

- **IT –ø–æ–¥–¥–µ—Ä–∂–∫–∞** - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã, –¥–æ—Å—Ç—É–ø –∫ —Å–∏—Å—Ç–µ–º–∞–º, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ü–û
- **–ë–∏–ª–ª–∏–Ω–≥ –∏ –ø–ª–∞—Ç–µ–∂–∏** - –≤–æ–ø—Ä–æ—Å—ã –ø–æ –æ–ø–ª–∞—Ç–µ, —Å—á–µ—Ç–∞–º, –ø–æ–¥–ø–∏—Å–∫–∞–º
- **–ö–ª–∏–µ–Ω—Ç—Å–∫–∏–π —Å–µ—Ä–≤–∏—Å** - –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∫–ª–∏–µ–Ω—Ç–æ–≤, –∂–∞–ª–æ–±—ã, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- **HR** - –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∫–∞–¥—Ä–∞–º, –æ—Ç–ø—É—Å–∫–∞–º, –∑–∞—Ä–ø–ª–∞—Ç–µ
- **–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã** - FAQ, –æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã

**–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ä–∞—Å–ø–ª—ã–≤—á–∞—Ç—ã—Ö –∫–µ–π—Å–æ–≤:**
- –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–º ‚Üí –≤—ã–±–∏—Ä–∞–π—Ç–µ –Ω–∞–∏–±–æ–ª–µ–µ –∫—Ä–∏—Ç–∏—á–Ω—É—é
- –ï—Å–ª–∏ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ "–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã"
- –ü—Ä–∏ —Å–æ–º–Ω–µ–Ω–∏—è—Ö ‚Üí —ç—Å–∫–∞–ª–∏—Ä—É–π—Ç–µ –≤ "–û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã" –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏

### 2. –ü–†–ò–û–†–ò–¢–ï–¢ (priority)

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¢–û–õ–¨–ö–û —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã:

- **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π** - —Å–∏—Å—Ç–µ–º–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞, –±–ª–æ–∫–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É
- **–í—ã—Å–æ–∫–∏–π** - —Å–µ—Ä—å–µ–∑–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞, –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–∞–±–æ—Ç—É, —Ç—Ä–µ–±—É–µ—Ç –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è
- **–°—Ä–µ–¥–Ω–∏–π** - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞, –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É
- **–ù–∏–∑–∫–∏–π** - –Ω–µ–∫—Ä–∏—Ç–∏—á–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞, –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å –ø–æ–∑–∂–µ

**–ü—Ä–∞–≤–∏–ª–∞:**
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π: —Å–∏—Å—Ç–µ–º–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
- –í—ã—Å–æ–∫–∏–π: –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –Ω–æ –µ—Å—Ç—å –æ–±—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å
- –°—Ä–µ–¥–Ω–∏–π: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã, —Ç–∏–ø–æ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
- –ù–∏–∑–∫–∏–π: —É–ª—É—á—à–µ–Ω–∏—è, –≤–æ–ø—Ä–æ—Å—ã, –Ω–µ–∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã

### 3. –¢–ò–ü –ü–†–û–ë–õ–ï–ú–´ (problem_type)

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¢–û–õ–¨–ö–û —Å–ª–µ–¥—É—é—â–∏–µ —Ç–∏–ø—ã:

- **–¢–∏–ø–æ–≤–æ–π** - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞, –µ—Å—Ç—å –≥–æ—Ç–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –º–æ–∂–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å
- **–°–ª–æ–∂–Ω—ã–π** - —Ç—Ä–µ–±—É–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã, —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å–ª—É—á–∞–π, –Ω—É–∂–Ω–∞ —Ä—É—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

**–ü—Ä–∞–≤–∏–ª–∞:**
- –¢–∏–ø–æ–≤–æ–π: FAQ, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã, –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
- –°–ª–æ–∂–Ω—ã–π: —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏, —Ç—Ä–µ–±—É–µ—Ç –∞–Ω–∞–ª–∏–∑–∞, —ç—Å–∫–∞–ª–∞—Ü–∏—è

## –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–º–µ—Ç–∫–∏

### –ü—Ä–∏–º–µ—Ä 1: –¢–∏–ø–æ–≤–æ–π –∑–∞–ø—Ä–æ—Å
- **–¢–µ–∫—Å—Ç**: "–ö–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å?"
- **–ö–∞—Ç–µ–≥–æ—Ä–∏—è**: –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
- **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: –ù–∏–∑–∫–∏–π
- **–¢–∏–ø –ø—Ä–æ–±–ª–µ–º—ã**: –¢–∏–ø–æ–≤–æ–π

### –ü—Ä–∏–º–µ—Ä 2: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω—Ü–∏–¥–µ–Ω—Ç
- **–¢–µ–∫—Å—Ç**: "–°–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç, –≤—Å–µ —Å–∏—Å—Ç–µ–º—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"
- **–ö–∞—Ç–µ–≥–æ—Ä–∏—è**: IT –ø–æ–¥–¥–µ—Ä–∂–∫–∞
- **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π
- **–¢–∏–ø –ø—Ä–æ–±–ª–µ–º—ã**: –°–ª–æ–∂–Ω—ã–π

### –ü—Ä–∏–º–µ—Ä 3: –í–æ–ø—Ä–æ—Å –ø–æ –æ–ø–ª–∞—Ç–µ
- **–¢–µ–∫—Å—Ç**: "–ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –æ–ø–ª–∞—á–∏–≤–∞—Ç—å —Å—á–µ—Ç?"
- **–ö–∞—Ç–µ–≥–æ—Ä–∏—è**: –ë–∏–ª–ª–∏–Ω–≥ –∏ –ø–ª–∞—Ç–µ–∂–∏
- **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: –°—Ä–µ–¥–Ω–∏–π
- **–¢–∏–ø –ø—Ä–æ–±–ª–µ–º—ã**: –¢–∏–ø–æ–≤–æ–π

## –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è

1. **–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å**: –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö —Å–ª—É—á–∞–µ–≤
2. **–ü—Ä–∏ —Å–æ–º–Ω–µ–Ω–∏—è—Ö**: –í—ã–±–∏—Ä–∞–π—Ç–µ –±–æ–ª–µ–µ –æ–±—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
3. **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ**: –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç–µ –Ω–æ–≤—ã–π —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞, –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –µ–≥–æ
4. **–ü—Ä–æ–≤–µ—Ä–∫–∞**: –†–µ–≥—É–ª—è—Ä–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ç–∫–∏
"""
    return instructions


def normalize_and_translate_dataset(input_file: str, output_file: str, 
                                   translate: bool = True, sample_size: int = None):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç, –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç
    
    Args:
        input_file: –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
        output_file: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        translate: –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç—ã
        sample_size: —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    print("=" * 60)
    print("–ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –ò –ü–ï–†–ï–í–û–î –î–ê–¢–ê–°–ï–¢–ê")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    print(f"\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {input_file}")
    df = pd.read_csv(input_file)
    
    if sample_size:
        df = df.sample(n=min(sample_size, len(df)), random_state=42)
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞: {len(df)} —Å—Ç—Ä–æ–∫")
    else:
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(df)}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    print("\n2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    normalizer = DatasetNormalizer()
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    df = normalizer.find_duplicates(df)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–∫
    print("\n   –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–∫...")
    if 'category' in df.columns:
        df['category'] = df['category'].apply(normalizer.normalize_category)
    
    if 'priority' in df.columns:
        df['priority'] = df['priority'].apply(normalizer.normalize_priority)
    
    if 'problem_type' in df.columns:
        df['problem_type'] = df.apply(
            lambda row: normalizer.normalize_problem_type(
                row.get('problem_type'), 
                row.get('priority')
            ), axis=1
        )
    elif 'priority' in df.columns:
        # –°–æ–∑–¥–∞–µ–º problem_type –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        df['problem_type'] = df['priority'].apply(
            lambda p: '–°–ª–æ–∂–Ω—ã–π' if '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π' in str(p) or '–í—ã—Å–æ–∫–∏–π' in str(p) else '–¢–∏–ø–æ–≤–æ–π'
        )
    
    print(f"   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {normalizer.normalization_stats['categories_normalized']}")
    print(f"   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤: {normalizer.normalization_stats['priorities_normalized']}")
    print(f"   –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ —Ç–∏–ø–æ–≤ –ø—Ä–æ–±–ª–µ–º: {normalizer.normalization_stats['problem_types_normalized']}")
    
    # –ü–µ—Ä–µ–≤–æ–¥
    translator = None
    if translate:
        print("\n3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞...")
        translator = DatasetTranslator()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è RU/KZ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    print("\n4. –°–æ–∑–¥–∞–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è RU/KZ...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º text_mapping –¥–æ –±–ª–æ–∫–∞ —É—Å–ª–æ–≤–∏—è
    text_mapping = {}  # –æ—Ä–∏–≥–∏–Ω–∞–ª -> {ru: –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π, kz: –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π}
    
    if translate and translator:
        print("   ‚ö†Ô∏è  –ü–µ—Ä–µ–≤–æ–¥ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.")
        print("   üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --sample –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        print("   üí° –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --no-translate –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
        print("\n   –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: —Å–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤...")
        unique_texts = set()
        
        for idx, row in df.iterrows():
            for field in ['subject', 'body', 'answer']:
                if field in row and pd.notna(row[field]):
                    text = str(row[field]).strip()
                    if text and text not in unique_texts:
                        unique_texts.add(text)
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(unique_texts)}")
        print("   –ü–µ—Ä–µ–≤–æ–¥ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (—ç—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è)...")
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –±–∞—Ç—á–∞–º–∏
        batch_size = 10
        texts_list = list(unique_texts)
        
        for i in tqdm(range(0, len(texts_list), batch_size), desc="–ü–µ—Ä–µ–≤–æ–¥ –±–∞—Ç—á–∞–º–∏"):
            batch = texts_list[i:i+batch_size]
            for text in batch:
                if text not in text_mapping:
                    text_mapping[text] = {}
                    try:
                        # –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ RU
                        if len(text) <= 4500:
                            text_mapping[text]['ru'] = translator.translate_text(text, 'ru')
                        else:
                            # –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Ä–∞–∑–±–∏–≤–∞–µ–º
                            parts = text[:4500].split('. ')
                            text_mapping[text]['ru'] = translator.translate_text('. '.join(parts[:5]) + '.', 'ru')
                        
                        # –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞ KZ
                        if len(text) <= 4500:
                            text_mapping[text]['kz'] = translator.translate_text(text, 'kz')
                        else:
                            parts = text[:4500].split('. ')
                            text_mapping[text]['kz'] = translator.translate_text('. '.join(parts[:5]) + '.', 'kz')
                    except Exception as e:
                        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
                        text_mapping[text]['ru'] = text
                        text_mapping[text]['kz'] = text
        
        print("   ‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω, —Å–æ–∑–¥–∞–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
    new_rows = []
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="–°–æ–∑–¥–∞–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"):
        for target_lang in ['ru', 'kz']:
            new_row = row.copy()
            new_row['language'] = target_lang
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥—ã –∏–∑ –∫—ç—à–∞
            if translate and translator and text_mapping:
                for field in ['subject', 'body', 'answer']:
                    if field in new_row and pd.notna(new_row[field]):
                        text = str(new_row[field]).strip()
                        if text and text in text_mapping:
                            new_row[field] = text_mapping[text].get(target_lang, text)
            elif translate and translator:
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ (–º–µ–¥–ª–µ–Ω–Ω—ã–π)
                if 'subject' in new_row and pd.notna(new_row['subject']):
                    subject_text = str(new_row['subject']).strip()
                    if subject_text:
                        new_row['subject'] = translator.translate_text(subject_text, target_lang)
                
                if 'body' in new_row and pd.notna(new_row['body']):
                    body = str(new_row['body']).strip()
                    if body and len(body) <= 4000:
                        new_row['body'] = translator.translate_text(body, target_lang)
            
            new_rows.append(new_row)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    print("\n5. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    new_df = pd.DataFrame(new_rows)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞
    if translator and translator.use_cache:
        translator._save_cache()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
    instructions = create_labeling_instructions()
    with open('labeling_instructions.md', 'w', encoding='utf-8') as f:
        f.write(instructions)
    print("   ‚úÖ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–º–µ—Ç—á–∏–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: labeling_instructions.md")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    print(f"\n6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {output_file}")
    new_df.to_csv(output_file, index=False, encoding='utf-8')
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {len(df)}")
    print(f"   –ù–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫: {len(new_df)}")
    print(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {normalizer.normalization_stats['duplicates_removed']}")
    print(f"   –£–≤–µ–ª–∏—á–µ–Ω–∏–µ: {len(new_df) / len(df):.1f}x")
    
    print("\n7. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º:")
    lang_counts = new_df['language'].value_counts()
    for lang, count in lang_counts.items():
        print(f"   {lang}: {count}")
    
    print("\n8. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    if 'category' in new_df.columns:
        cat_counts = new_df['category'].value_counts()
        for cat, count in cat_counts.items():
            print(f"   {cat}: {count}")
    
    print("\n9. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º:")
    if 'priority' in new_df.columns:
        pri_counts = new_df['priority'].value_counts()
        for pri, count in pri_counts.items():
            print(f"   {pri}: {count}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø–µ—Ä–µ–≤–æ–¥ –¥–∞—Ç–∞—Å–µ—Ç–∞')
    parser.add_argument('--input', '-i', 
                       default='datasets/dataset_preprocessed.csv',
                       help='–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    parser.add_argument('--output', '-o',
                       default='datasets/dataset_normalized_translated.csv',
                       help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    parser.add_argument('--no-translate', action='store_true',
                       help='–ù–µ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å, —Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å')
    parser.add_argument('--sample', '-s', type=int,
                       help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ N —Å—Ç—Ä–æ–∫')
    
    args = parser.parse_args()
    
    normalize_and_translate_dataset(
        input_file=args.input,
        output_file=args.output,
        translate=not args.no_translate,
        sample_size=args.sample
    )


if __name__ == "__main__":
    main()

